python3 NewsGroup_NB.py
/usr/local/lib64/python3.6/site-packages/sklearn/feature_extraction/text.py:1817: UserWarning: Only (<class 'numpy.float64'>, <class 'numpy.float32'>, <class 'numpy.float16'>) 'dtype' should be used. int32 'dtype' will be converted to np.float64.
  UserWarning)

Vectorized all data shape:  (18846, 20000)

Vectorized training data shape:  (11314, 20000)

Vectorized training data shape:  (7532, 20000)
model fitting started
Starting model Prediction

NewsGroup Multinomial Naive Bayes Model Classification Accuracy Score: 0.6024960169941582

Classification Report:               precision    recall  f1-score   support

           0       0.80      0.08      0.14       319
           1       0.67      0.65      0.66       389
           2       0.73      0.54      0.62       394
           3       0.60      0.76      0.67       392
           4       0.84      0.58      0.69       385
           5       0.78      0.73      0.75       395
           6       0.84      0.76      0.80       390
           7       0.82      0.64      0.72       396
           8       0.92      0.59      0.72       398
           9       0.95      0.70      0.80       397
          10       0.57      0.89      0.69       399
          11       0.56      0.76      0.64       396
          12       0.75      0.41      0.53       393
          13       0.83      0.61      0.70       396
          14       0.85      0.62      0.71       394
          15       0.20      0.94      0.33       398
          16       0.62      0.58      0.60       364
          17       0.84      0.70      0.76       376
          18       0.86      0.08      0.15       310
          19       0.50      0.00      0.01       251

    accuracy                           0.60      7532
   macro avg       0.73      0.58      0.59      7532
weighted avg       0.73      0.60      0.61      7532

NewsGroup Multinomial Naive Bayes Model Confusion Matrix: 
 [[ 24   1   0   1   0   3   0   0   2   0   9   4   0   4   1 258   1  10
    0   1]
 [  0 254  16  11   9  20   1   1   0   0   5  19   2   1   5  43   1   1
    0   0]
 [  0  20 213  50   6  19   0   0   2   0  15  13   0   3   8  43   1   1
    0   0]
 [  0   5  22 296   9   3   9   0   0   2   8  13  13   0   0  12   0   0
    0   0]
 [  0  10   9  62 224   3   8   4   1   0  14  14   7   2   1  24   1   1
    0   0]
 [  0  29  15   8   1 289   3   1   0   0   7  16   2   1   1  22   0   0
    0   0]
 [  0   2   3  22   9   2 295   7   1   0  11   2   5   0   3  25   3   0
    0   0]
 [  0   0   1   5   0   3  10 255   7   0  26   8  12   3   4  58   2   1
    1   0]
 [  0   1   0   2   0   1   7  17 236   4  16  14   4   9   2  80   2   3
    0   0]
 [  0   6   1   0   0   7   1   0   1 277  34   5   1   2   0  62   0   0
    0   0]
 [  0   0   0   0   0   1   0   1   0   4 354   2   0   1   1  34   1   0
    0   0]
 [  0  10   2   2   1   6   1   0   2   0  17 300   1   1   2  40   8   3
    0   0]
 [  0  15   5  31   6   6  11   9   2   1  14  65 162   8   7  48   1   2
    0   0]
 [  0  10   1   1   1   4   2   1   0   0  17   5   2 240   0 108   2   2
    0   0]
 [  0   9   0   0   0   1   1   3   0   1  19  13   3   4 243  90   3   4
    0   0]
 [  0   2   1   0   0   1   0   0   0   0  15   1   0   1   1 376   0   0
    0   0]
 [  0   0   2   0   0   0   1   3   1   1  14  19   0   0   2 103 210   6
    2   0]
 [  0   1   1   0   0   1   0   1   1   2   8   4   0   2   0  84   6 264
    1   0]
 [  0   1   0   0   0   1   0   4   0   0  10  20   1   4   4 149  81  10
   25   0]
 [  6   3   0   0   0   1   0   3   0   0   9   3   0   2   2 196  17   8
    0   1]]

HYPERPARAMETRE TUNING
Fitting 3 folds for each of 4 candidates, totalling 12 fits
[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:  1.2min finished
>>>>> Optimized params
{'alpha': 0.0001}
>>>>>> Display the top results of grid search cv
   mean_fit_time  std_fit_time  mean_score_time  std_score_time  ... split2_test_score mean_test_score  std_test_score  rank_test_score
2       4.632022      0.064795         1.503918        0.017501  ...          0.761602        0.764629        0.009527                1
1       4.593570      0.007194         1.477609        0.002280  ...          0.763988        0.762596        0.008045                2
3       4.638393      0.036518         1.492604        0.011388  ...          0.741448        0.747658        0.009302                3
0       4.641641      0.029662         1.523790        0.055825  ...          0.610448        0.600938        0.007751                4

[4 rows x 12 columns]
Using our training-dataset optimized multinomial naive bayes model on the testing dataset for evaluating
score = 0.725836

